{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import random\n",
    "import IPython\n",
    "from IPython.display import Image, Audio\n",
    "import music21\n",
    "from music21 import *\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter\n",
    "import os\n",
    "\n",
    "filepath = \"./albeniz/\"\n",
    "# Getting midi files\n",
    "all_midis = []\n",
    "for i in os.listdir(filepath):\n",
    "    if i.endswith(\".mid\"):\n",
    "        tr = os.path.join(filepath, i)  # Ensures the correct path joining\n",
    "        midi = converter.parse(tr)\n",
    "        all_midis.append(midi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a library like music21 to extract notes and chords from the MIDI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import converter, instrument, note, chord\n",
    "\n",
    "notes = []\n",
    "for midi_file in all_midis:\n",
    "    parts = midi_file.getElementsByClass('Part')  # Use 'Part' in quotes to fetch the class\n",
    "    for part in parts:\n",
    "        elements = part.recurse()  # Recursively get all elements\n",
    "        for element in elements:\n",
    "            if isinstance(element, note.Note):  # If element is a Note\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):  # If element is a Chord\n",
    "                # Join normalOrder to create a representation of the chord\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sequences of notes/chords into input/output pairs for training the LSTM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical  # Correct import\n",
    "\n",
    "# Encode the notes\n",
    "label_encoder = LabelEncoder()\n",
    "int_notes = label_encoder.fit_transform(notes)\n",
    "\n",
    "sequence_length = 100\n",
    "input_sequences = []\n",
    "output_sequences = []\n",
    "\n",
    "for i in range(len(int_notes) - sequence_length):\n",
    "    input_sequences.append(int_notes[i:i + sequence_length])\n",
    "    output_sequences.append(int_notes[i + sequence_length])\n",
    "\n",
    "# Reshape and normalize\n",
    "input_sequences = np.reshape(input_sequences, (len(input_sequences), sequence_length, 1))\n",
    "input_sequences = input_sequences / float(len(set(int_notes)))\n",
    "\n",
    "# One-hot encode the output sequences\n",
    "output_sequences = to_categorical(output_sequences)  # Correct function name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an LSTM model using Keras or TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',   # Metric to monitor\n",
    "    patience=10,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore the weights of the best model after stopping\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(input_sequences.shape[1], input_sequences.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(set(notes))))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "validation_split = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 195ms/step - loss: 4.8000 - val_loss: 4.9174\n",
      "Epoch 2/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 197ms/step - loss: 4.4224 - val_loss: 5.0908\n",
      "Epoch 3/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 199ms/step - loss: 4.3209 - val_loss: 5.3505\n",
      "Epoch 4/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 198ms/step - loss: 4.2078 - val_loss: 5.2016\n",
      "Epoch 5/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 208ms/step - loss: 4.1633 - val_loss: 5.2825\n",
      "Epoch 6/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 224ms/step - loss: 4.0182 - val_loss: 5.2803\n",
      "Epoch 7/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 224ms/step - loss: 3.9299 - val_loss: 5.3948\n",
      "Epoch 8/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 210ms/step - loss: 3.7964 - val_loss: 5.4263\n",
      "Epoch 9/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 204ms/step - loss: 3.6625 - val_loss: 5.6374\n",
      "Epoch 10/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 204ms/step - loss: 3.4832 - val_loss: 5.7484\n",
      "Epoch 11/100\n",
      "\u001b[1m219/219\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 207ms/step - loss: 3.3388 - val_loss: 5.7552\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    input_sequences, output_sequences,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    validation_split=validation_split,  # Specify the percentage of data for validation\n",
    "    callbacks=[early_stopping]          # Include the early stopping callback\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Randomly select a starting index\n",
    "start_index = random.randint(0, len(input_sequences) - 1)\n",
    "seed = input_sequences[start_index]\n",
    "\n",
    "generated_notes = []\n",
    "\n",
    "for i in range(500):  # Length of the generated music\n",
    "    prediction = model.predict(seed, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "\n",
    "    # Ensure the index is within the valid range of labels\n",
    "    if index >= len(label_encoder.classes_):\n",
    "        index = len(label_encoder.classes_) - 1  # Clip to the max valid index\n",
    "\n",
    "    predicted_note = label_encoder.inverse_transform([index])[0]\n",
    "    generated_notes.append(predicted_note)\n",
    "\n",
    "    # Update the seed with the predicted note, keeping the sequence length constant\n",
    "    seed = np.append(seed[1:], [[index]], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import stream, note, chord, instrument\n",
    "\n",
    "def create_midi(predicted_notes, output_file=\"output.mid\"):\n",
    "    # Create a music21 stream object\n",
    "    midi_stream = stream.Stream()\n",
    "\n",
    "    # Add an instrument part (optional)\n",
    "    midi_stream.append(instrument.Piano())  # You can choose different instruments\n",
    "\n",
    "    # Loop through the predicted sequence and convert to notes/chords\n",
    "    for pattern in predicted_notes:\n",
    "        # If the pattern is a chord (e.g., '60.64.67'), create a Chord\n",
    "        if '.' in pattern or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes_in_chord = [int(n) for n in notes_in_chord]  # Convert to integers\n",
    "            new_chord = chord.Chord(notes_in_chord)\n",
    "            new_chord.duration.quarterLength = 0.5  # Set duration (can be adjusted)\n",
    "            midi_stream.append(new_chord)\n",
    "        else:  # If the pattern is a single note, create a Note\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.duration.quarterLength = 0.5  # Set duration (can be adjusted)\n",
    "            midi_stream.append(new_note)\n",
    "\n",
    "    # Write the stream to a MIDI file\n",
    "    midi_stream.write('midi', fp=output_file)\n",
    "\n",
    "# Example usage (replace 'generated_notes' with your actual predicted notes sequence)\n",
    "create_midi(predicted_notes=generated_notes, output_file=\"generated_music.mid\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
